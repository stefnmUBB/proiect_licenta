\par A vector is a tool that enables processing of multiple quantities at once \cite{IntroVec}. Let $x_1, x_2, \ldots x_n \in \mathbb{K}$ be $n$ values in a field $(\mathbb{K},+,\cdot)$. Using these values we can build an $n$-dimensional vector $\mathrm{x}$ described as a tuple $\mathrm{x} = (x_1, \ldots,x_n)\in\mathbb{K}^n$. In the following we consider $\mathbb{K} = \mathbb{R}$ and highlight the most usual vector properties and operations.
The addition of two vectors $\mathrm{x}$ and $\mathrm{y}$ in $\mathbb{R}^n$ is defined as the vector obtained from summing each component of one vector with its counterpart from the other vector:
$$\mathrm{x}+\mathrm{y} = (x_1,\ldots,x_n)+(y_1,\ldots,y_n)=(x_1+y_1,\ldots,x_n+y_n).$$
Obviously, vector addition is commutative: $\mathrm{x}+\mathrm{y}=\mathrm{y}+\mathrm{x} \ \forall \mathrm{x},\mathrm{y}\in\mathbb{R}^n $, and associative: $(\mathrm{x}+\mathrm{y})+\mathrm{z}=\mathrm{x}+(\mathrm{y}+\mathrm{z}) \ \forall \mathrm{x},\mathrm{y},\mathrm{z}\in\mathbb{R}^n $. The vector $\mathrm{0}=(0,...0)\in \mathbb{R}^n$ is the neutral element with respect to addition ($\mathrm{x}+\mathrm{0}=\mathrm{x}\ \forall \mathrm{x} \in\mathbb{R}^n $) and for each vector $\mathrm{x}$ there exists an inverse vector $-\mathrm{x}=(-x_1,\ldots,-x_n)$ such that $\mathrm{x}+(-\mathrm{x})=\mathrm{0}$ \cite{IntroVec}.

Let $\alpha\in\mathbb{R}$ be a scalar and $\mathrm{x}\in\mathbb{R}^n$ a vector. The vector $\alpha\mathrm{x}=(\alpha x_1,\ldots,\alpha x_n)$ is called the 
product of the vector $\mathrm{x}$ with the scalar $\alpha$. The usual properties of multiplication \cite{IntroVec} hold:
\begin{itemize}
\item associativity: $(\alpha\beta)\mathrm{x} = \alpha(\beta\mathrm{x}), \beta\in \mathbb{R}$;
\item identity: $\mathrm{1}\mathrm{x}=\mathrm{x}$;
\item distributivity of vector: $(\alpha+\beta)\mathrm{}{x}=\alpha\mathrm{x}+\beta\mathrm{x}$
\item distributivity of scalar:
$\alpha(\mathrm{x}+\mathrm{y})=\alpha\mathrm{x}+\alpha\mathrm{y}, \mathrm{y}\in\mathbb{R}^n$
\end{itemize} 

\par Given a vector $\mathrm{x}\in\mathbb{R}^n$, we define the magnitude of the vector $|\mathrm{x}|$ to be the euclidean norm of its components:
$$ |\mathrm{x}| = \sqrt{(x_1^2 + \ldots + x_n^2)}. $$
It can be easily verified that for $\alpha\in\mathbb{R}$, $|\alpha \mathrm{x}|=|\alpha|\cdot|\mathrm{x}|$ \cite{IntroVec}. A vector with magnitude $1$ is called unit (or direction) vector. 

\par The interesting part arises when we try to imagine the product of two vectors. Some popular definitions are scalar and vector products. We are focusing on the scalar product, as it is more relevant for the current work. There are two useful formulations of the scalar product $\mathrm{x}\cdot\mathrm{y}$ (also called inner, or dot product \cite{IntroVec}) of two vectors $\mathrm{x},\mathrm{y}\in\mathbb{R}^n$. One of them is defined in function of their components:
$$\mathrm{x}\cdot\mathrm{y}=\sum_{i=1}^{n}{x_i y_i}=x_1 y_1 + \ldots + x_n y_n.$$
The other one emerges from the geometric interpretation of the scalar product as being the length of the projection of one vector to the other:
$$\mathrm{x}\cdot\mathrm{y} = |\mathrm{x}||\mathrm{y}| \cos{\angle{(\mathrm{x},\mathrm{y})}}.$$
This last relation, stripped of its geometric meaning, inspires one of the applications of the scalar product as an expression of similarity between two vectors, describing how much one diverges from another by measuring the angle between them \cite{cossim}.

\par We call a matrix $A \in\mathcal{M}_{m,n}(\mathbb{R}), m,n\in\mathbb{N}^\star$ a collection of $m \times n$ real scalars displayed in a tabular grid consisting of $n$ rows and $m$ columns:
$$ A = \begin{bmatrix}
    a_{11}       & a_{12} & a_{13} & \dots & a_{1m} \\
    a_{21}       & a_{22} & a_{23} & \dots & a_{2m} \\
    \hdotsfor{5} \\
    a_{n1}       & a_{n2} & a_{n3} & \dots & a_{nm}
\end{bmatrix},$$
where the element $a_{i j}$ is placed at row $i$, column $j$ \cite{LinAlSchaum}.
The short notation of the matrix $A$ using its elements $a_{i j}, 1 \leq i \leq n, 1 \leq j \leq m$ is $A=(a_{i j})_{1 \leq i \leq n, 1 \leq j \leq m }$.
Note that a vector can be visualized as a matrix where one of the dimensions is equal to $1$. Hence, the vector $(x_1,\ldots,x_n)\in\mathbb{R}^n$ can be viewed either as a 
one-line matrix $ \begin{bmatrix} x_{1} & x_{2} & \dots & x_{n}  \end{bmatrix}\in \mathcal{M}_{1,n}(\mathbb{R}) $, or as a one-column matrix
$ \begin{bmatrix} x_{1} & x_{2} & \dots & x_{n}  \end{bmatrix}^T = \begin{bmatrix} x_{1} \\ \vdots \\ x_{n}  \end{bmatrix}^T \in \mathcal{M}_{n,1}(\mathbb{R}) $.

\par The matrix addition and scalar multiplication are defined analogously to the vector operations and share the same properties \cite{LinAlSchaum}. For matrices $A=(a_{i j}),B=(b_{i j}) \in\mathcal{M}_{m,n}(\mathbb{R})$ and scalar $\alpha \in \mathbb{R}$, we have:
\begin{itemize}
\item $A+B = (a_{i j} + b_{i j})_{(i,j)\in\mathbb{R}^2 \cap ([1,n]\times[1,m])}$;
\item $\alpha A = (\alpha a_{i j})_{(i,j)\in\mathbb{R}^2 \cap ([1,n]\times[1,m])}$.
\end{itemize} 

\par Now let's consider two matrices $A \in\mathcal{M}_{m,n}(\mathbb{R}), B \in\mathcal{M}_{n,p}(\mathbb{R}), n,m,p \in \mathbb{N}^\star$. The product of the two matrix is the matrix $C = AB \in \mathcal{M}_{m,p}$, where
$$c_{i j} = \sum_{k=0}^{m} {a_{i k} b_{k j}}. $$
Note the analogy with the vectors' scalar product. In fact, when $m=p=1$, the matrix multiplication between a column vector and a row vector produces in fact the scalar product of these two vectors.

\par So far we have considered a vector to be an ordered collection of scalars, and a matrix to be a stacked collection of vectors. We can extend this process and infer the concept of stacked matrices of same type to form a 3-dimensional structure of scalars, and we can continue like that on higher dimensions. We will refer to these structures as tensors. For matters of simplicity, we will skip over some rigorous mathematical aspects of tensors \cite{TensorsPorat_2014} like variance and emphasize on the more intuitive properties that are useful in this work from a computational point of view. We therefore consider a tensor to be a collection of scalars with multidimensional index access. Let $n$ be the number of dimensions of the tensor and $d_1, d_2, \ldots, d_n \in \mathbb{N}$ the length of each dimension. The number $n$ is called the rank of the tensor, while the tuple $d=(d_1,\ldots,d_n)$ is the shape of the tensor. For a tensor $t$ of rank $n$ and dimension $d$, we note $t_{i_1 i_2 \ldots i_n}$ the component contained at indices $1 \leq i_j \leq d_j, j= \overline{1,n}$. When $n=0$, we call it a shapeless tensor, and it roughly behaves like a scalar value. We introduce the notation $\mathcal{T}_{n}(d)=\{t \ | \ t \ \mathrm{is} \ a \ \mathrm{tensor} \ \mathrm{of} \ \mathrm{rank} \ n \ \mathrm{and} \ \mathrm{shape} \ d, d=(d_1,...,d_n)\}$ the set of $d$-shaped tensors, and $\mathcal{D}(d)=\prod_{k=1}^{n}{[1,d_k]}\times \mathbb{N}^n$ the set of all possible indices tuples corresponding to shape $d$. We use the term "axis $k$ of a rank $n$ tensor", $1 \leq k \leq n$, to refer the the $k$-th dimension of its shape. For example, in a matrix treated as a rank 2 tensor, axis 1 names the rows indices, while axis 2 describes the column indices. The length of an axis $k$ is $d_k$, the largest possible index value of that dimension. In the following, we will describe some commonly used operations with tensors \cite{D2l}.

\par Reshaping \cite{TensorsAsMDArrays} a tensor means altering its shape in such a way that it preserves the number of components and their order when iterating along all dimensions. A tensor $A$ of shape $d=(d_1,\ldots,d_n)$ can change its shape to $d'=(d'_1,\ldots,d'_m)$ if $\prod_{k=1}^{n}{d_k}=\prod_{k=1}^{m}{d'_k}$. In particular, a $0$-rank tensor can be reshaped to any shape whose components counts equals to $1$: $(1)$, $(1,1)$, $(1,1,1)$ etc. The reverse statement holds.

\par Concatenation of two tensors $A\in\mathcal{T}_n((d_1, \ldots, d_{k-1},d_k, d_{k+1},\ldots,d_n))$ and $B\in\mathcal{T}_n((d_1, \ldots,$ $d_{k-1},d'_k, d_{k+1},\ldots,d_n))$ along an axis $k\in\{1,\ldots,n\}$ implies obtaining the tensor $C=\mathrm{concat}(A,B,k)\in\mathcal{T}((d_1, \ldots, d_{k-1},d_k+d'_k, d_{k+1},\ldots,d_n))$ by extending the axis $k$ of $A$ with values from $B$. The tensor $C$ has its components
$$c_{i_1,\ldots,i_{k-1},i_{k},i_{k+1},\ldots,i_n}=\begin{cases}
  a_{i_1,\ldots,i_{k-1},i_{k},i_{k+1}} & \text{if $i_k \leq d_k$} \\
  b_{i_1,\ldots,i_{k-1},i_{k}-d_k,i_{k+1}} & \text{if $i_k > d_k$} 
\end{cases}.$$
A particular use of concatenation is tensor stacking along an axis $k$, $\mathrm{stack}_{k}(A_1, A_2, \ldots, A_m)$ where $A_\#$ are tensors of the same shape $d=(d_1,\ldots,d_n), n>=k-1$. the stacking operation is defined as follows
$$\mathrm{stack}_{k}(A_1, A_2, \ldots, A_m)=\substack{\maltese \\ \mathrm{concat}}_{i=1}^{m}(\mathrm{reshape}(A_i, (d_1,\ldots,d_{k-1}, 1, d_{k},\ldots,d_n))),$$
where $\substack{\maltese \\ \mathrm{f}}$ denotes aggregation by the two variable function $f$: 
$$\substack{\maltese \\ \mathrm{f}}(X)=X, 
\substack{\maltese \\ \mathrm{f}}_{i=1}^{n} X_i = f(\substack{\maltese \\ \mathrm{f}}_{i=1}^{n-1}X_i, X_n).$$
\par The inverse operation is called unstacking and involves splitting a rank-$n$ tensor into a list of as many rank-$(n-1)$ tensors as the length of the target dimension.

\par Element-wise operations are performed on tensors of the same shape $d$. Considering an operator $ \bullet : \mathbb{R} \times \mathbb{R} \rightarrow \mathbb{R}$ defined on scalar pairs, We can extend its functionality on tensors $ \bullet_T : \mathcal{T}_{n}(d) \times \mathcal{T}_{n}(d) \rightarrow \mathcal{T}_{n}(d)$, $A \bullet_T B = C$, where $c_i = a_i \bullet b_i, i \in \mathcal{D}(d)$. The operator $\bullet$ can represent any usual arithmetic operation. In particular, when $\bullet=\cdot$ real multiplication operator and $n=2$, the operator $\bullet_T = \circledcirc$  is called the Hadamard product \cite{D2l}, the element by element multiplication performed on matrices, $A \circledcirc B = (a_{i j} b_{i j})_{(i,j)\in \mathcal{D}(d)}$. Generalizing further, we can make the operator $\bullet_T$ allow arguments of different shapes: $\bullet_T : \mathcal{T}(d_1) \times \mathcal{T}(d_2)\leftarrow \mathcal{T}(d_r)$. By choosing, for example, $d_1=()$, $a \bullet B$ is an operation between a scalar and a tensor and can be viewed as streaming the scalar value to each component of the tensor. It is virtually equivalent to creating as many copies of the scalar $a$ as the components count of tensor $B$ and placing them in the same shape as the tensor. Thus, $a \bullet_T B$ reduces to calculating $A \bullet_T B$, where $A=(a)_{i\in \mathcal{D}(d_2)}$. A similar approach can be performed for $d_1=(1)$. Let's consider now two arbitrary shapes $d_1$ and $d_2$ of the same rank and for each axis $k$, either $d_{1 k} = d_{2 k}$ or one of $d_{1 k}$ for ${d_{2 k}}$ is equal to ${1}$. Therefore, we are building the output of $A \bullet_T B$ as the tensor $C \in \mathcal{T}(d_r)$ with $d_r=(\max{\{d_{1 i}, {d_{2 i}\}}})_{i=\overline{1,n}}, c_{i_1 \ldots i_n} = a_{\theta_a} \bullet b_{\theta_b}$, where $\theta_a = (\min{(i_k, d_{1 k})})_{k=\overline{1,n}}$ and $\theta_b = (\min{(i_k, d_{2 k})})_{k=\overline{1,n}}$. If $d1$ and $d2$ are of different shapes, one can consider reshaping the tensor of lower rank to by prepending axis of length 1 until it reaches the same rank as the other tensor and applying the steps above. This process is known as tensor broadcasting \cite{Numpy}.

Similar to broadcasting, we can introduce batch operations \cite{Numpy}. Let $f:\mathcal{T}_n(d)\rightarrow\mathcal{T}_r(e)$ be a mapping function from one tensor space to another. Let $A\in\mathcal{T}_m({\delta})$ be a tensor of rank $m \geq n$ and $\delta_{m-i} = d_{n-i}, \forall \ 0\leq i \leq n-1$. Then $f(A)$ can be computed by applying $f$ on each rank-{n} subtensor $\alpha_i, i=(i_1,i_2,\ldots,i_{m-n}), 1\leq i_k \leq \delta_{k}$, $(\alpha_i)_{j_1,\ldots,j_n} = a_{i_1,\ldots,i_{m-n}, j_1,\ldots,j_n}$ and rejoining the results, possibly by iterative stacking, into a tensor of shape $(\delta_1,\ldots,\delta_{m-n},e_1,\ldots,e_r)$ and rank $m-n+r$. A concrete illustration on this concept would be considering $A$ a $(b,n,n)$-shaped tensor interpreted as $b$ stacked $n \times n$ and $f$ a function that operates on matrices (for example, the $f=\det$). The result of $f(A)$ would be a tensor of shape $(b)$, visualized as a vector where component at index $i$ represents the determinant of the matrix described by the values of $A_{i,\#,\#}$. Hence we have batch-applied the determinant on a tensor viewed as a list of matrices.